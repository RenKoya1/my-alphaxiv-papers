{
  "data": [
    {
      "_id": "6851b878e4d7cd843caec1c4",
      "user_id": "6851b878e4d7cd843caec1c8",
      "name": "Want to read",
      "type": "default-to-read",
      "order": 4,
      "papers": [],
      "created_at": "2025-06-17T18:48:24.772Z",
      "updated_at": "2025-06-17T18:48:24.772Z",
      "id": "01977f38-97c4-7d7c-8f95-c4923289002e"
    },
    {
      "_id": "6851b878e4d7cd843caec1c5",
      "user_id": "6851b878e4d7cd843caec1c8",
      "name": "Reading",
      "type": "default-reading",
      "order": 3,
      "papers": [
        {
          "paper_id": "689d4897ea57ba53fca8be98",
          "added_at": "2025-08-19T01:49:45.033Z",
          "details": {
            "_id": "689d4897ea57ba53fca8be98",
            "subcategories": [
              "cs.CV"
            ],
            "citation": {
              "bibtex": "@misc{li2025seeinglisteningremembering,\n      title={Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory},\n      author={Wei Li and Hang Li and Yuan Lin and Junbo Zhao and Wentao Ye and Lin Long and Yichen He and Yiyuan Pan},\n      year={2025},\n      eprint={2508.09736},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.09736},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f7e",
                "name": "Shanghai Jiao Tong University",
                "aliases": []
              },
              {
                "_id": "67be6376aa92218ccd8b0fa4",
                "name": "Zhejiang University",
                "aliases": [],
                "image": "images/organizations/zhejiang.png"
              },
              {
                "_id": "67be6377aa92218ccd8b0fe7",
                "name": "ByteDance",
                "aliases": [
                  "ByteDance Seed",
                  "ByteDance Inc.",
                  "ByteDance Research",
                  "ByteDance Inc",
                  "ByteDance China",
                  "Bytedance Inc",
                  "Bytedance Seed",
                  "Bytedance",
                  "bytedance",
                  "Bytedance Inc."
                ],
                "image": "images/organizations/bytedance.png"
              }
            ],
            "id": "689d4897ea57ba53fca8be98",
            "paper_id": "2508.09736",
            "title": "Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory",
            "abstract": "We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memory. Beyond episodic memory, it also develops semantic memory, enabling it to accumulate world knowledge over time. Its memory is organized in an entity-centric, multimodal format, allowing deeper and more consistent understanding of the environment. Given an instruction, M3-Agent autonomously performs multi-turn, iterative reasoning and retrieves relevant information from memory to accomplish the task. To evaluate memory effectiveness and memory-based reasoning in multimodal agents, we develop M3-Bench, a new long-video question answering benchmark. M3-Bench comprises 100 newly recorded real-world videos captured from a robot's perspective (M3-Bench-robot) and 920 web-sourced videos across diverse scenarios (M3-Bench-web). We annotate question-answer pairs designed to test key capabilities essential for agent applications, such as human understanding, general knowledge extraction, and cross-modal reasoning. Experimental results show that M3-Agent, trained via reinforcement learning, outperforms the strongest baseline, a prompting agent using Gemini-1.5-pro and GPT-4o, achieving 6.7%, 7.7%, and 5.3% higher accuracy on M3-Bench-robot, M3-Bench-web and VideoMME-long, respectively. Our work advances the multimodal agents toward more human-like long-term memory and provides insights into their practical design. Model, code and data are available at this https URL",
            "authors": [
              "Wei Li",
              "Hang Li",
              "Yuan Lin",
              "Junbo Zhao",
              "Wentao Ye",
              "Lin Long",
              "Yichen He",
              "Yiyuan Pan"
            ],
            "publication_date": "2025-08-15T13:40:53.000Z",
            "imageURL": "image/2508.09736v2.png",
            "public_total_votes": 255
          }
        },
        {
          "paper_id": "689e91426bc81f8d09d651ac",
          "added_at": "2025-08-19T03:00:45.657Z",
          "details": {
            "_id": "689e91426bc81f8d09d651ac",
            "subcategories": [
              "cs.CL",
              "cs.AI",
              "cs.LG"
            ],
            "organizationInfo": [],
            "id": "689e91426bc81f8d09d651ac",
            "paper_id": "2508.10419",
            "title": "ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning",
            "abstract": "Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given the LLM's diminished reasoning over extended context and high computational cost, retrieval-based approaches remain a pivotal role in practice. However, traditional RAG methods can fall short due to their stateless, single-step retrieval process, which often overlooks the dynamic nature of capturing interconnected relations within long-range context. In this work, we propose ComoRAG, holding the principle that narrative reasoning is not a one-shot process, but a dynamic, evolving interplay between new evidence acquisition and past knowledge consolidation, analogous to human cognition when reasoning with memory-related signals in the brain. Specifically, when encountering a reasoning impasse, ComoRAG undergoes iterative reasoning cycles while interacting with a dynamic memory workspace. In each cycle, it generates probing queries to devise new exploratory paths, then integrates the retrieved evidence of new aspects into a global memory pool, thereby supporting the emergence of a coherent context for the query resolution. Across four challenging long-context narrative benchmarks (200K+ tokens), ComoRAG outperforms strong RAG baselines with consistent relative gains up to 11% compared to the strongest baseline. Further analysis reveals that ComoRAG is particularly advantageous for complex queries requiring global comprehension, offering a principled, cognitively motivated paradigm for retrieval-based long context comprehension towards stateful reasoning. Our code is publicly released at this https URL",
            "authors": [
              "Jie Zhou",
              "Wei Wei",
              "Jin Xu",
              "Liyan Xu",
              "Mo Yu",
              "Yufeng Wang",
              "Juyuan Wang",
              "Rongchen Zhao"
            ],
            "publication_date": "2025-08-14T07:52:09.000Z",
            "imageURL": "image/2508.10419v1.png",
            "public_total_votes": 20
          }
        },
        {
          "paper_id": "689e93586bc81f8d09d651dd",
          "added_at": "2025-08-19T14:53:56.056Z",
          "details": {
            "_id": "689e93586bc81f8d09d651dd",
            "subcategories": [
              "cs.CV",
              "cs.LG"
            ],
            "citation": {
              "bibtex": "@misc{jégou2025dinov3,\n      title={DINOv3},\n      author={Hervé Jégou and Julien Mairal and Piotr Bojanowski and Francisco Massa and Jianyuan Wang and Maxime Oquab and Timothée Darcet and Théo Moutakanni and Marc Szafraniec and Vasil Khalidov and Daniel Haziza and Patrick Labatut and Andrea Vedaldi and Oriane Siméoni and Camille Couprie and Huy V. Vo and Luca Wehrstedt and Michaël Ramamonjisoa and Maximilian Seitzer and Cijo Jose and Federico Baldassarre and Jamie Tolan and John Brandt and Claire Roberts and Seungeun Yi and Leonel Sentana},\n      year={2025},\n      eprint={2508.10104},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.10104},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be637caa92218ccd8b120e",
                "name": "Inria",
                "aliases": []
              },
              {
                "_id": "67be6653aa92218ccd8b6652",
                "name": "Meta AI Research",
                "aliases": []
              },
              {
                "_id": "689e936585c754a102bf8c16",
                "name": "WRI",
                "aliases": []
              }
            ],
            "id": "689e93586bc81f8d09d651dd",
            "paper_id": "2508.10104",
            "title": "DINOv3",
            "abstract": "Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored to specific tasks or domains, this training paradigm has the potential to learn visual representations from diverse sources, ranging from natural to aerial images -- using a single algorithm. This technical report introduces DINOv3, a major milestone toward realizing this vision by leveraging simple yet effective strategies. First, we leverage the benefit of scaling both dataset and model size by careful data preparation, design, and optimization. Second, we introduce a new method called Gram anchoring, which effectively addresses the known yet unsolved issue of dense feature maps degrading during long training schedules. Finally, we apply post-hoc strategies that further enhance our models' flexibility with respect to resolution, model size, and alignment with text. As a result, we present a versatile vision foundation model that outperforms the specialized state of the art across a broad range of settings, without fine-tuning. DINOv3 produces high-quality dense features that achieve outstanding performance on various vision tasks, significantly surpassing previous self- and weakly-supervised foundation models. We also share the DINOv3 suite of vision models, designed to advance the state of the art on a wide spectrum of tasks and data by providing scalable solutions for diverse resource constraints and deployment scenarios.",
            "authors": [
              "Hervé Jégou",
              "Julien Mairal",
              "Piotr Bojanowski",
              "Francisco Massa",
              "Jianyuan Wang",
              "Maxime Oquab",
              "Timothée Darcet",
              "Théo Moutakanni",
              "Marc Szafraniec",
              "Vasil Khalidov",
              "Daniel Haziza",
              "Patrick Labatut",
              "Andrea Vedaldi",
              "Oriane Siméoni",
              "Camille Couprie",
              "Huy V. Vo",
              "Luca Wehrstedt",
              "Michaël Ramamonjisoa",
              "Maximilian Seitzer",
              "Cijo Jose",
              "Federico Baldassarre",
              "Jamie Tolan",
              "John Brandt",
              "Claire Roberts",
              "Seungeun Yi",
              "Leonel Sentana"
            ],
            "publication_date": "2025-08-13T18:00:55.000Z",
            "imageURL": "image/2508.10104v1.png",
            "public_total_votes": 175
          }
        },
        {
          "paper_id": "689d3bf9ea57ba53fca8bcc2",
          "added_at": "2025-08-19T02:38:00.055Z",
          "details": {
            "_id": "689d3bf9ea57ba53fca8bcc2",
            "subcategories": [
              "cs.CL",
              "cs.AI"
            ],
            "citation": {
              "bibtex": "@misc{chen2025memorydecoderpretrained,\n      title={Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models},\n      author={Kai Chen and Qipeng Guo and Bowen Zhou and Zhouhan Lin and Jiarui Wang and Jiaqi Cao and Rubin Wei},\n      year={2025},\n      eprint={2508.09874},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.09874},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f6f",
                "name": "Tsinghua University",
                "aliases": [],
                "image": "images/organizations/tsinghua.png"
              },
              {
                "_id": "67be6376aa92218ccd8b0f7e",
                "name": "Shanghai Jiao Tong University",
                "aliases": []
              },
              {
                "_id": "67be6377aa92218ccd8b1019",
                "name": "Shanghai AI Laboratory",
                "aliases": []
              }
            ],
            "id": "689d3bf9ea57ba53fca8bcc2",
            "paper_id": "2508.09874",
            "title": "Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models",
            "abstract": "Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.",
            "authors": [
              "Kai Chen",
              "Qipeng Guo",
              "Bowen Zhou",
              "Zhouhan Lin",
              "Jiarui Wang",
              "Jiaqi Cao",
              "Rubin Wei"
            ],
            "publication_date": "2025-08-13T15:16:29.000Z",
            "imageURL": "image/2508.09874v1.png",
            "public_total_votes": 50
          }
        },
        {
          "paper_id": "689be1e52db7e5b4fb831044",
          "added_at": "2025-08-19T16:24:52.576Z",
          "details": {
            "_id": "689be1e52db7e5b4fb831044",
            "subcategories": [
              "cs.CL",
              "cs.SE"
            ],
            "citation": {
              "bibtex": "@misc{zhang2025autocodebenchlargelanguage,\n      title={AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators},\n      author={Tao Zhang and Ao Liu and Yue Mao and Fengzong Lian and Haotian Zhu and Chenchen Zhang and Jason Chou and Yuchi Deng and Ziyan Xu and Jianwei Cai and Bohui Zhai and Lingyun Tan and Wiggin Zhou and Zhiying Zeng and Hengyi Liu and Speed Zhu},\n      year={2025},\n      eprint={2508.09101},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.09101},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6383aa92218ccd8b1406",
                "name": "Tencent",
                "aliases": [],
                "image": "images/organizations/tencent.png"
              }
            ],
            "id": "689be1e52db7e5b4fb831044",
            "paper_id": "2508.09101",
            "title": "AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to evaluate their code generation abilities, these benchmarks face several critical limitations. First, they often rely on manual annotations, which are time-consuming and difficult to scale across different programming languages and problem complexities. Second, most existing benchmarks focus primarily on Python, while the few multilingual benchmarks suffer from limited difficulty and uneven language distribution. To address these challenges, we propose AutoCodeGen, an automated method for generating high-difficulty multilingual code generation datasets without manual annotations. AutoCodeGen ensures the correctness and completeness of test cases by generating test inputs with LLMs and obtaining test outputs through a multilingual sandbox, while achieving high data quality through reverse-order problem generation and multiple filtering steps. Using this novel method, we introduce AutoCodeBench, a large-scale code generation benchmark comprising 3,920 problems evenly distributed across 20 programming languages. It is specifically designed to evaluate LLMs on challenging, diverse, and practical multilingual tasks. We evaluate over 30 leading open-source and proprietary LLMs on AutoCodeBench and its simplified version AutoCodeBench-Lite. The results show that even the most advanced LLMs struggle with the complexity, diversity, and multilingual nature of these tasks. Besides, we introduce AutoCodeBench-Complete, specifically designed for base models to assess their few-shot code generation capabilities. We hope the AutoCodeBench series will serve as a valuable resource and inspire the community to focus on more challenging and practical multilingual code generation scenarios.",
            "authors": [
              "Tao Zhang",
              "Ao Liu",
              "Yue Mao",
              "Fengzong Lian",
              "Haotian Zhu",
              "Chenchen Zhang",
              "Jason Chou",
              "Yuchi Deng",
              "Ziyan Xu",
              "Jianwei Cai",
              "Bohui Zhai",
              "Lingyun Tan",
              "Wiggin Zhou",
              "Zhiying Zeng",
              "Hengyi Liu",
              "Speed Zhu"
            ],
            "publication_date": "2025-08-12T17:29:20.000Z",
            "imageURL": "image/2508.09101v1.png",
            "public_total_votes": 21
          }
        },
        {
          "paper_id": "689c05a3afc9da4145a2a0c8",
          "added_at": "2025-08-15T05:43:43.190Z",
          "details": {
            "_id": "689c05a3afc9da4145a2a0c8",
            "subcategories": [
              "cs.RO",
              "cs.MA"
            ],
            "citation": {
              "bibtex": "@misc{li2025deepfleetmultiagentfoundation,\n      title={DeepFleet: Multi-Agent Foundation Models for Mobile Robots},\n      author={Ang Li and Scott Niekum and Kyle O'Brien and Usman A. Khan and Joseph W. Durham and Federico Pecora and Sriram Siva and Ameya Agaskar and Brianna Gallo Sarker and Dino Kirouani and William Pickering and Charles Kekeh and Alicia Chua and Mayur Nemade and Charun Thattai and Jiaming Di and Isaac Iyengar and Ramya Dharoor and Jimmy Erskine and Tamir Hegazy},\n      year={2025},\n      eprint={2508.08574},\n      archivePrefix={arXiv},\n      primaryClass={cs.RO},\n      url={https://arxiv.org/abs/2508.08574},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be64f2aa92218ccd8b46e9",
                "name": "Amazon Robotics",
                "aliases": []
              }
            ],
            "id": "689c05a3afc9da4145a2a0c8",
            "paper_id": "2508.08574",
            "title": "DeepFleet: Multi-Agent Foundation Models for Mobile Robots",
            "abstract": "We introduce DeepFleet, a suite of foundation models designed to support coordination and planning for large-scale mobile robot fleets. These models are trained on fleet movement data, including robot positions, goals, and interactions, from hundreds of thousands of robots in Amazon warehouses worldwide. DeepFleet consists of four architectures that each embody a distinct inductive bias and collectively explore key points in the design space for multi-agent foundation models: the robot-centric (RC) model is an autoregressive decision transformer operating on neighborhoods of individual robots; the robot-floor (RF) model uses a transformer with cross-attention between robots and the warehouse floor; the image-floor (IF) model applies convolutional encoding to a multi-channel image representation of the full fleet; and the graph-floor (GF) model combines temporal attention with graph neural networks for spatial relationships. In this paper, we describe these models and present our evaluation of the impact of these design choices on prediction task performance. We find that the robot-centric and graph-floor models, which both use asynchronous robot state updates and incorporate the localized structure of robot interactions, show the most promise. We also present experiments that show that these two models can make effective use of larger warehouses operation datasets as the models are scaled up.",
            "authors": [
              "Ang Li",
              "Scott Niekum",
              "Kyle O'Brien",
              "Usman A. Khan",
              "Joseph W. Durham",
              "Federico Pecora",
              "Sriram Siva",
              "Ameya Agaskar",
              "Brianna Gallo Sarker",
              "Dino Kirouani",
              "William Pickering",
              "Charles Kekeh",
              "Alicia Chua",
              "Mayur Nemade",
              "Charun Thattai",
              "Jiaming Di",
              "Isaac Iyengar",
              "Ramya Dharoor",
              "Jimmy Erskine",
              "Tamir Hegazy"
            ],
            "publication_date": "2025-08-12T02:19:15.000Z",
            "imageURL": "image/2508.08574v1.png",
            "public_total_votes": 9
          }
        },
        {
          "paper_id": "689559d617fe5423a59ee670",
          "added_at": "2025-08-10T14:37:26.104Z",
          "details": {
            "_id": "689559d617fe5423a59ee670",
            "subcategories": [
              "cs.LG"
            ],
            "citation": {
              "bibtex": "@misc{yang2025generalizationsftreinforcement,\n      title={On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification},\n      author={Ming-Hsuan Yang and Xinyu Ye and Lu Qi and Xu Yang and Zhou Ziheng and Wenbo Zhu and Yizhou Zhou and Xinting Hu and Yongliang Wu and Yingzhe Peng},\n      year={2025},\n      eprint={2508.05629},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG},\n      url={https://arxiv.org/abs/2508.05629},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f7e",
                "name": "Shanghai Jiao Tong University",
                "aliases": []
              },
              {
                "_id": "67be6376aa92218ccd8b0f83",
                "name": "UC Berkeley",
                "aliases": [
                  "University of California, Berkeley",
                  "UC-Berkeley",
                  "Simons Institute for the Theory of Computing, University of California, Berkeley",
                  "The Simons Institute for the Theory of Computing at UC Berkeley"
                ],
                "image": "images/organizations/berkeley.png"
              },
              {
                "_id": "67be6377aa92218ccd8b1034",
                "name": "Wuhan University",
                "aliases": []
              },
              {
                "_id": "67be6378aa92218ccd8b1096",
                "name": "University of California, Los Angeles",
                "aliases": []
              },
              {
                "_id": "67be6379aa92218ccd8b10c5",
                "name": "Nanyang Technological University",
                "aliases": []
              },
              {
                "_id": "67be6379aa92218ccd8b10d8",
                "name": "Southeast University",
                "aliases": []
              },
              {
                "_id": "67be637caa92218ccd8b11de",
                "name": "University of California, Merced",
                "aliases": []
              }
            ],
            "id": "689559d617fe5423a59ee670",
            "paper_id": "2508.05629",
            "title": "On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification",
            "abstract": "We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning (RL). Through mathematical analysis, we reveal that standard SFT gradients implicitly encode a problematic reward structure that may severely restrict the generalization capabilities of model. To rectify this, we propose Dynamic Fine-Tuning (DFT), stabilizing gradient updates for each token by dynamically rescaling the objective function with the probability of this token. Remarkably, this single-line code change significantly outperforms standard SFT across multiple challenging benchmarks and base models, demonstrating greatly improved generalization. Additionally, our approach shows competitive results in offline RL settings, offering an effective yet simpler alternative. This work bridges theoretical insight and practical solutions, substantially advancing SFT performance. The code will be available at this https URL.",
            "authors": [
              "Ming-Hsuan Yang",
              "Xinyu Ye",
              "Lu Qi",
              "Xu Yang",
              "Zhou Ziheng",
              "Wenbo Zhu",
              "Yizhou Zhou",
              "Xinting Hu",
              "Yongliang Wu",
              "Yingzhe Peng"
            ],
            "publication_date": "2025-08-07T17:59:04.000Z",
            "imageURL": "image/2508.05629v1.png",
            "public_total_votes": 203
          }
        },
        {
          "paper_id": "6892b22c551f444de8ee7c7b",
          "added_at": "2025-08-10T15:51:24.029Z",
          "details": {
            "_id": "6892b22c551f444de8ee7c7b",
            "subcategories": [
              "cs.AI",
              "cs.LG"
            ],
            "citation": {
              "bibtex": "@misc{yang2025agentlightningtrain,\n      title={Agent Lightning: Train ANY AI Agents with Reinforcement Learning},\n      author={Yuqing Yang and Zilong Wang and Dongsheng Li and Zhiyuan He and Xufang Luo and Siyun Zhao and Luna K. Qiu and Yuge Zhang},\n      year={2025},\n      eprint={2508.03680},\n      archivePrefix={arXiv},\n      primaryClass={cs.AI},\n      url={https://arxiv.org/abs/2508.03680},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6379aa92218ccd8b10f6",
                "name": "Microsoft",
                "aliases": [
                  "Microsoft Azure",
                  "Microsoft GSL",
                  "Microsoft Corporation",
                  "Microsoft Research",
                  "Microsoft Research Asia",
                  "Microsoft Research Montreal",
                  "Microsoft Research AI for Science",
                  "Microsoft India",
                  "Microsoft Research Redmond",
                  "Microsoft Spatial AI Lab",
                  "Microsoft Azure Research",
                  "Microsoft Research India",
                  "Microsoft Research AI4Science",
                  "Microsoft AI for Good Research Lab",
                  "Microsoft Research Cambridge",
                  "Microsoft Corporaion"
                ],
                "image": "images/organizations/microsoft.png"
              }
            ],
            "id": "6892b22c551f444de8ee7c7b",
            "paper_id": "2508.03680",
            "title": "Agent Lightning: Train ANY AI Agents with Reinforcement Learning",
            "abstract": "We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that tightly couple RL training with agent or rely on sequence concatenation with masking, Agent Lightning achieves complete decoupling between agent execution and training, allowing seamless integration with existing agents developed via diverse ways (e.g., using frameworks like LangChain, OpenAI Agents SDK, AutoGen, and building from scratch) with almost ZERO code modifications. By formulating agent execution as Markov decision process, we define an unified data interface and propose a hierarchical RL algorithm, LightningRL, which contains a credit assignment module, allowing us to decompose trajectories generated by ANY agents into training transition. This enables RL to handle complex interaction logic, such as multi-agent scenarios and dynamic workflows. For the system design, we introduce a Training-Agent Disaggregation architecture, and brings agent observability frameworks into agent runtime, providing a standardized agent finetuning interface. Experiments across text-to-SQL, retrieval-augmented generation, and math tool-use tasks demonstrate stable, continuous improvements, showcasing the framework's potential for real-world agent training and deployment.",
            "authors": [
              "Yuqing Yang",
              "Zilong Wang",
              "Dongsheng Li",
              "Zhiyuan He",
              "Xufang Luo",
              "Siyun Zhao",
              "Luna K. Qiu",
              "Yuge Zhang"
            ],
            "publication_date": "2025-08-05T17:50:13.000Z",
            "imageURL": "image/2508.03680v1.png",
            "public_total_votes": 179
          }
        },
        {
          "paper_id": "68799d921821b4d35ad898ef",
          "added_at": "2025-08-19T03:14:13.580Z",
          "details": {
            "_id": "68799d921821b4d35ad898ef",
            "subcategories": [
              "cs.CL"
            ],
            "citation": {
              "bibtex": "@misc{li2025surveycontextengineering,\n      title={A Survey of Context Engineering for Large Language Models},\n      author={Mingyu Li and Yujun Cai and Jiayu Yao and Jiafeng Guo and Jiayi Mao and Chenlin Zhou and Lingrui Mei and Duzhen Zhang and Shenghua Liu and Yiwei Wang and Baolong Bi and Yuyao Ge and Zhong-Zhi Li and Jiazhi Liu and Tianze Xia},\n      year={2025},\n      eprint={2507.13334},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2507.13334},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f68",
                "name": "University of Chinese Academy of Sciences",
                "aliases": []
              },
              {
                "_id": "67be6376aa92218ccd8b0f6f",
                "name": "Tsinghua University",
                "aliases": [],
                "image": "images/organizations/tsinghua.png"
              },
              {
                "_id": "67be6377aa92218ccd8b0fc7",
                "name": "Institute of Computing Technology, Chinese Academy of Sciences",
                "aliases": []
              },
              {
                "_id": "67be6377aa92218ccd8b0ff5",
                "name": "Peking University",
                "aliases": [],
                "image": "images/organizations/peking.png"
              },
              {
                "_id": "67be637caa92218ccd8b11de",
                "name": "University of California, Merced",
                "aliases": []
              },
              {
                "_id": "67be638aaa92218ccd8b15ef",
                "name": "The University of Queensland",
                "aliases": []
              }
            ],
            "id": "68799d921821b4d35ad898ef",
            "paper_id": "2507.13334",
            "title": "A Survey of Context Engineering for Large Language Models",
            "abstract": "The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline that transcends simple prompt design to encompass the systematic optimization of information payloads for LLMs. We present a comprehensive taxonomy decomposing Context Engineering into its foundational components and the sophisticated implementations that integrate them into intelligent systems. We first examine the foundational components: context retrieval and generation, context processing and context management. We then explore how these components are architecturally integrated to create sophisticated system implementations: retrieval-augmented generation (RAG), memory systems and tool-integrated reasoning, and multi-agent systems. Through this systematic analysis of over 1400 research papers, our survey not only establishes a technical roadmap for the field but also reveals a critical research gap: a fundamental asymmetry exists between model capabilities. While current models, augmented by advanced context engineering, demonstrate remarkable proficiency in understanding complex contexts, they exhibit pronounced limitations in generating equally sophisticated, long-form outputs. Addressing this gap is a defining priority for future research. Ultimately, this survey provides a unified framework for both researchers and engineers advancing context-aware AI.",
            "authors": [
              "Mingyu Li",
              "Yujun Cai",
              "Jiayu Yao",
              "Jiafeng Guo",
              "Jiayi Mao",
              "Chenlin Zhou",
              "Lingrui Mei",
              "Duzhen Zhang",
              "Shenghua Liu",
              "Yiwei Wang",
              "Baolong Bi",
              "Yuyao Ge",
              "Zhong-Zhi Li",
              "Jiazhi Liu",
              "Tianze Xia"
            ],
            "publication_date": "2025-07-21T17:48:18.000Z",
            "imageURL": "image/2507.13334v2.png",
            "public_total_votes": 270
          }
        },
        {
          "paper_id": "6837d2685142e4a4d6e04928",
          "added_at": "2025-08-10T17:03:43.400Z",
          "details": {
            "_id": "6837d2685142e4a4d6e04928",
            "subcategories": [
              "cs.CL"
            ],
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f6a",
                "name": "Renmin University of China",
                "aliases": [],
                "image": "images/organizations/renmin.png"
              },
              {
                "_id": "67be6376aa92218ccd8b0f7e",
                "name": "Shanghai Jiao Tong University",
                "aliases": []
              },
              {
                "_id": "67c0fa359fdf15298df1ddcb",
                "name": "Research Institute of China Telecom",
                "aliases": []
              },
              {
                "_id": "67fdc05de6bb3a9c6236d8a6",
                "name": "MemTensor (Shanghai) Technology Co., Ltd.",
                "aliases": []
              }
            ],
            "id": "6837d2685142e4a4d6e04928",
            "paper_id": "2505.22101",
            "title": "MemOS: An Operating System for Memory-Augmented Generation (MAG) in  Large Language Models",
            "abstract": "Large Language Models (LLMs) have emerged as foundational infrastructure in\nthe pursuit of Artificial General Intelligence (AGI). Despite their remarkable\ncapabilities in language perception and generation, current LLMs fundamentally\nlack a unified and structured architecture for handling memory. They primarily\nrely on parametric memory (knowledge encoded in model weights) and ephemeral\nactivation memory (context-limited runtime states). While emerging methods like\nRetrieval-Augmented Generation (RAG) incorporate plaintext memory, they lack\nlifecycle management and multi-modal integration, limiting their capacity for\nlong-term knowledge evolution. To address this, we introduce MemOS, a memory\noperating system designed for LLMs that, for the first time, elevates memory to\na first-class operational resource. It builds unified mechanisms for\nrepresentation, organization, and governance across three core memory types:\nparametric, activation, and plaintext. At its core is the MemCube, a\nstandardized memory abstraction that enables tracking, fusion, and migration of\nheterogeneous memory, while offering structured, traceable access across tasks\nand contexts. MemOS establishes a memory-centric execution framework with\nstrong controllability, adaptability, and evolvability. It fills a critical gap\nin current LLM infrastructure and lays the groundwork for continual adaptation,\npersonalized intelligence, and cross-platform coordination in next-generation\nintelligent systems.",
            "authors": [
              "Jiawei Yang",
              "Zhi-Qin John Xu",
              "Kai Chen",
              "Tianyi Chen",
              "Bo Tang",
              "Hanyu Wang",
              "Shichao Song",
              "Qingchen Yu",
              "Feiyu Xiong",
              "Zhiyu Li",
              "Jiahao Huo",
              "Jihao Zhao",
              "Simin Niu",
              "Junpeng Ren",
              "Ding Chen",
              "Hongkang Yang",
              "Zehao Lin",
              "Chenyang Xi",
              "Yezhaohui Wang",
              "Huayi Lai",
              "Kehang Li",
              "Zhiqiang Yin"
            ],
            "publication_date": "2025-05-28T08:27:12.000Z",
            "imageURL": "image/2505.22101v1.png",
            "public_total_votes": 73
          }
        },
        {
          "paper_id": "681475896543b462f4d596d9",
          "added_at": "2025-08-10T15:54:43.209Z",
          "details": {
            "_id": "681475896543b462f4d596d9",
            "subcategories": [
              "cs.CL"
            ],
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f71",
                "name": "The Chinese University of Hong Kong",
                "aliases": [],
                "image": "images/organizations/chinesehongkong.png"
              },
              {
                "_id": "67be6377aa92218ccd8b100b",
                "name": "The University of Edinburgh",
                "aliases": []
              },
              {
                "_id": "67be6379aa92218ccd8b10d2",
                "name": "HKUST",
                "aliases": [],
                "image": "images/organizations/hkust.jpg"
              },
              {
                "_id": "68147d2a2dd9b125510eef69",
                "name": "Huawei UK R&D Ltd.",
                "aliases": []
              }
            ],
            "id": "681475896543b462f4d596d9",
            "paper_id": "2505.00675",
            "title": "Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future  Directions",
            "abstract": "Memory is a fundamental component of AI systems, underpinning large language\nmodels (LLMs)-based agents. While prior surveys have focused on memory\napplications with LLMs (e.g., enabling personalized memory in conversational\nagents), they often overlook the atomic operations that underlie memory\ndynamics. In this survey, we first categorize memory representations into\nparametric and contextual forms, and then introduce six fundamental memory\noperations: Consolidation, Updating, Indexing, Forgetting, Retrieval, and\nCompression. We map these operations to the most relevant research topics\nacross long-term, long-context, parametric modification, and multi-source\nmemory. By reframing memory systems through the lens of atomic operations and\nrepresentation types, this survey provides a structured and dynamic perspective\non research, benchmark datasets, and tools related to memory in AI, clarifying\nthe functional interplay in LLMs based agents while outlining promising\ndirections for future research\\footnote{The paper list, datasets, methods and\ntools are available at\n\\href{https://github.com/Elvin-Yiming-Du/Survey_Memory_in_AI}{this https URL}.}.",
            "authors": [
              "Danna Zheng",
              "Mirella Lapata",
              "Jeff Z. Pan",
              "Kam-Fai Wong",
              "Zhaowei Wang",
              "Yiming Du",
              "Wenyu Huang",
              "Sebastien Montella"
            ],
            "publication_date": "2025-05-27T17:38:40.000Z",
            "imageURL": "image/2505.00675v2.png",
            "public_total_votes": 820
          }
        },
        {
          "paper_id": "68102c2d152d4e7dfe3ec58e",
          "added_at": "2025-08-10T14:27:19.481Z",
          "details": {
            "_id": "68102c2d152d4e7dfe3ec58e",
            "subcategories": [
              "cs.CL",
              "cs.AI"
            ],
            "organizationInfo": [],
            "id": "68102c2d152d4e7dfe3ec58e",
            "paper_id": "2504.19413",
            "title": "Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory",
            "abstract": "Large Language Models (LLMs) have demonstrated remarkable prowess in\ngenerating contextually coherent responses, yet their fixed context windows\npose fundamental challenges for maintaining consistency over prolonged\nmulti-session dialogues. We introduce Mem0, a scalable memory-centric\narchitecture that addresses this issue by dynamically extracting,\nconsolidating, and retrieving salient information from ongoing conversations.\nBuilding on this foundation, we further propose an enhanced variant that\nleverages graph-based memory representations to capture complex relational\nstructures among conversational elements. Through comprehensive evaluations on\nLOCOMO benchmark, we systematically compare our approaches against six baseline\ncategories: (i) established memory-augmented systems, (ii) retrieval-augmented\ngeneration (RAG) with varying chunk sizes and k-values, (iii) a full-context\napproach that processes the entire conversation history, (iv) an open-source\nmemory solution, (v) a proprietary model system, and (vi) a dedicated memory\nmanagement platform. Empirical results show that our methods consistently\noutperform all existing memory systems across four question categories:\nsingle-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26%\nrelative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with\ngraph memory achieves around 2% higher overall score than the base\nconfiguration. Beyond accuracy gains, we also markedly reduce computational\noverhead compared to full-context method. In particular, Mem0 attains a 91%\nlower p95 latency and saves more than 90% token cost, offering a compelling\nbalance between advanced reasoning capabilities and practical deployment\nconstraints. Our findings highlight critical role of structured, persistent\nmemory mechanisms for long-term conversational coherence, paving the way for\nmore reliable and efficient LLM-driven AI agents.",
            "authors": [
              "Prateek Chhikara",
              "Deshraj Yadav",
              "Taranjeet Singh",
              "Dev Khant",
              "Saket Aryan"
            ],
            "publication_date": "2025-04-28T01:46:35.000Z",
            "imageURL": "image/2504.19413v1.png",
            "public_total_votes": 659
          }
        },
        {
          "paper_id": "673ccead7d2b7ed9dd51e076",
          "added_at": "2025-08-10T13:46:35.607Z",
          "details": {
            "_id": "673ccead7d2b7ed9dd51e076",
            "subcategories": [
              "cs.CV"
            ],
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f9b",
                "name": "Google DeepMind",
                "aliases": [
                  "DeepMind",
                  "Google Deepmind",
                  "Deepmind",
                  "Google DeepMind Robotics"
                ],
                "image": "images/organizations/deepmind.png"
              }
            ],
            "id": "673ccead7d2b7ed9dd51e076",
            "paper_id": "2402.05861",
            "title": "Memory Consolidation Enables Long-Context Video Understanding",
            "abstract": "Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the cost of both conceptual and computational complexity. We propose to instead re-purpose existing pre-trained video transformers by simply fine-tuning them to attend to memories derived non-parametrically from past activations. By leveraging redundancy reduction, our memory-consolidated vision transformer (MC-ViT) effortlessly extends its context far into the past and exhibits excellent scaling behavior when learning from longer videos. In doing so, MC-ViT sets a new state-of-the-art in long-context video understanding on EgoSchema, Perception Test, and Diving48, outperforming methods that benefit from orders of magnitude more parameters.",
            "authors": [
              "Rahma Chaabouni",
              "Yuge Shi",
              "Skanda Koppula",
              "Ivana Balažević",
              "Pinelopi Papalampidi",
              "Olivier J. Hénaff"
            ],
            "publication_date": "2024-05-31T15:22:58.000Z",
            "activity_score": 0,
            "imageURL": "image/2402.05861v2.png",
            "public_total_votes": 10
          }
        }
      ],
      "created_at": "2025-06-17T18:48:24.772Z",
      "updated_at": "2025-06-17T18:48:24.772Z",
      "id": "01977f38-97c4-74ae-9282-303c7cdbd3d0"
    },
    {
      "_id": "6851b878e4d7cd843caec1c6",
      "user_id": "6851b878e4d7cd843caec1c8",
      "name": "Completed",
      "type": "default-completed",
      "order": 2,
      "papers": [
        {
          "paper_id": "68a2825700372a33db1e86f6",
          "added_at": "2025-08-19T14:53:12.652Z",
          "details": {
            "_id": "68a2825700372a33db1e86f6",
            "subcategories": [
              "cs.CV"
            ],
            "citation": {
              "bibtex": "@misc{chen2025thymethinkbeyond,\n      title={Thyme: Think Beyond Images},\n      author={Wei Chen and Fan Yang and Liang Wang and Kaibing Chen and Bin Wen and Tianke Zhang and Changyi Liu and Guorui Zhou and Yi-Fan Zhang and Chaoyou Fu and Zhang Zhang and Xiao Hu and Xingyu Lu and Tingting Gao and Shukang Yin and Kaiyu Tang and Jiankang Chen and Haojie Ding and Kaiyu Jiang and Haonan Fan},\n      year={2025},\n      eprint={2508.11630},\n      archivePrefix={arXiv},\n      primaryClass={cs.CV},\n      url={https://arxiv.org/abs/2508.11630},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f6f",
                "name": "Tsinghua University",
                "aliases": [],
                "image": "images/organizations/tsinghua.png"
              },
              {
                "_id": "67be6376aa92218ccd8b0f8c",
                "name": "Nanjing University",
                "aliases": [],
                "image": "images/organizations/nanjing.png"
              },
              {
                "_id": "67be6377aa92218ccd8b1016",
                "name": "University of Science and Technology of China",
                "aliases": []
              },
              {
                "_id": "68661c8f2dac261d0fc6034a",
                "name": "Chinese Academy of Sciences Institute of Automation",
                "aliases": []
              },
              {
                "_id": "68a28263773bbee4a9992e13",
                "name": "Kwai Keye",
                "aliases": []
              }
            ],
            "id": "68a2825700372a33db1e86f6",
            "paper_id": "2508.11630",
            "title": "Thyme: Think Beyond Images",
            "abstract": "Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in perception and reasoning tasks. However, to the best of our knowledge, no open-source work currently offers a feature set as rich as proprietary models (O3), which can perform diverse image manipulations and simultaneously enhance logical reasoning capabilities through code. In this paper, we make a preliminary attempt in this direction by introducing Thyme (Think Beyond Images), a novel paradigm for enabling MLLMs to transcend existing ``think with images'' approaches by autonomously generating and executing diverse image processing and computational operations via executable code. This approach not only facilitates a rich, on-the-fly set of image manipulations (e.g., cropping, rotation, contrast enhancement) but also allows for mathematical computations, all while maintaining high autonomy in deciding when and how to apply these operations. We activate this capability through a two-stage training strategy: an initial SFT on a curated dataset of 500K samples to teach code generation, followed by a RL phase to refine decision-making. For the RL stage, we manually collect and design high-resolution question-answer pairs to increase the learning difficulty, and we propose GRPO-ATS (Group Relative Policy Optimization with Adaptive Temperature Sampling), an algorithm that applies distinct temperatures to text and code generation to balance reasoning exploration with code execution precision. We conduct extensive experimental analysis and ablation studies. Comprehensive evaluations on nearly 20 benchmarks show that Thyme yields significant and consistent performance gains, particularly in challenging high-resolution perception and complex reasoning tasks.",
            "authors": [
              "Wei Chen",
              "Fan Yang",
              "Liang Wang",
              "Kaibing Chen",
              "Bin Wen",
              "Tianke Zhang",
              "Changyi Liu",
              "Guorui Zhou",
              "Yi-Fan Zhang",
              "Chaoyou Fu",
              "Zhang Zhang",
              "Xiao Hu",
              "Xingyu Lu",
              "Tingting Gao",
              "Shukang Yin",
              "Kaiyu Tang",
              "Jiankang Chen",
              "Haojie Ding",
              "Kaiyu Jiang",
              "Haonan Fan"
            ],
            "publication_date": "2025-08-15T17:59:49.000Z",
            "imageURL": "image/2508.11630v1.png",
            "public_total_votes": 125
          }
        },
        {
          "paper_id": "689e8dcc6bc81f8d09d6515d",
          "added_at": "2025-08-19T14:53:24.744Z",
          "details": {
            "_id": "689e8dcc6bc81f8d09d6515d",
            "subcategories": [
              "cs.CL"
            ],
            "citation": {
              "bibtex": "@misc{zhang2025ssrlselfsearchreinforcement,\n      title={SSRL: Self-Search Reinforcement Learning},\n      author={Yuchen Zhang and Gang Chen and Ning Ding and Yuchen Fan and Xinwei Long and Kaiyan Zhang and Bowen Zhou and Lei Bai and Xuekai Zhu and Bingning Wang and Yu Fu and Che Jiang and Yuxin Zuo and Cheng Huang and Yanxu Chen and Heng Zhou and Li Kang and Zhizhou He},\n      year={2025},\n      eprint={2508.10874},\n      archivePrefix={arXiv},\n      primaryClass={cs.CL},\n      url={https://arxiv.org/abs/2508.10874},\n}"
            },
            "organizationInfo": [
              {
                "_id": "67be6376aa92218ccd8b0f6f",
                "name": "Tsinghua University",
                "aliases": [],
                "image": "images/organizations/tsinghua.png"
              },
              {
                "_id": "67be6376aa92218ccd8b0f7e",
                "name": "Shanghai Jiao Tong University",
                "aliases": []
              },
              {
                "_id": "67be6377aa92218ccd8b0fdb",
                "name": "University College London",
                "aliases": []
              },
              {
                "_id": "67be6377aa92218ccd8b1019",
                "name": "Shanghai AI Laboratory",
                "aliases": []
              },
              {
                "_id": "67be6611aa92218ccd8b6088",
                "name": "WeChat AI",
                "aliases": []
              },
              {
                "_id": "689e8dd1f429a9a350b42f2a",
                "name": "CSCEC Third Bureau",
                "aliases": []
              }
            ],
            "id": "689e8dcc6bc81f8d09d6515d",
            "paper_id": "2508.10874",
            "title": "SSRL: Self-Search Reinforcement Learning",
            "abstract": "We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interactions with external search engines. To this end, we first quantify the intrinsic search capability of LLMs via structured prompting and repeated sampling, which we term Self-Search. Our results reveal that LLMs exhibit strong scaling behavior with respect to the inference budget, achieving high pass@k on question-answering benchmarks, including the challenging BrowseComp task. Building on these observations, we introduce Self-Search RL (SSRL), which enhances LLMs' Self-Search capability through format-based and rule-based rewards. SSRL enables models to iteratively refine their knowledge utilization internally, without requiring access to external tools. Empirical evaluations demonstrate that SSRL-trained policy models provide a cost-effective and stable environment for search-driven RL training, reducing reliance on external search engines and facilitating robust sim-to-real transfer. We draw the following conclusions: 1) LLMs possess world knowledge that can be effectively elicited to achieve high performance; 2) SSRL demonstrates the potential of leveraging internal knowledge to reduce hallucination; 3) SSRL-trained models integrate seamlessly with external search engines without additional effort. Our findings highlight the potential of LLMs to support more scalable RL agent training.",
            "authors": [
              "Yuchen Zhang",
              "Gang Chen",
              "Ning Ding",
              "Yuchen Fan",
              "Xinwei Long",
              "Kaiyan Zhang",
              "Bowen Zhou",
              "Lei Bai",
              "Xuekai Zhu",
              "Bingning Wang",
              "Yu Fu",
              "Che Jiang",
              "Yuxin Zuo",
              "Cheng Huang",
              "Yanxu Chen",
              "Heng Zhou",
              "Li Kang",
              "Zhizhou He"
            ],
            "publication_date": "2025-08-14T17:46:01.000Z",
            "imageURL": "image/2508.10874v1.png",
            "public_total_votes": 84
          }
        }
      ],
      "created_at": "2025-06-17T18:48:24.772Z",
      "updated_at": "2025-06-17T18:48:24.772Z",
      "id": "01977f38-97c4-7640-a7af-4b539f8bc590"
    },
    {
      "_id": "6851b878e4d7cd843caec1c7",
      "user_id": "6851b878e4d7cd843caec1c8",
      "name": "My publications",
      "type": "default-publications",
      "order": 1,
      "papers": [],
      "created_at": "2025-06-17T18:48:24.772Z",
      "updated_at": "2025-06-17T18:48:24.772Z",
      "id": "01977f38-97c4-7c06-95ab-16842aa425c9"
    }
  ]
}
