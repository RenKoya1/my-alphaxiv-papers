# 📚 Research Papers Collection

> Automatically updated research paper collection organized by reading status

![Papers](https://img.shields.io/badge/Papers-16-blue) ![Folders](https://img.shields.io/badge/Folders-4-green) ![Updated](https://img.shields.io/badge/Updated-2025_08_22-orange)

## 📊 Quick Stats

| Metric | Count |
|--------|-------|
| 📁 Total Folders | 4 |
| 📄 Total Papers | 16 |
| 📚 Reading | 13 |
| ✅ Completed | 3 |

---

## 🏆 My publications

> No papers yet.

## ✅ Completed

<details open>
<summary><strong>3 papers</strong> - Click to expand</summary>

### 1. [Thyme: Think Beyond Images](https://arxiv.org/abs/2508.11630)

**Authors:** Wei Chen, Fan Yang, Liang Wang et al.  
**ArXiv ID:** `2508.11630`  
**Published:** 2025-08-15  
**Organizations:** Tsinghua University, Nanjing University et al.  
**Votes:** 155 votes  
**Abstract:** Following OpenAI's introduction of the ``thinking with images'' concept, recent efforts have explored stimulating the use of visual information in the reasoning process to enhance model performance in...

**Categories:** `cs.CV`

---

### 2. [SSRL: Self-Search Reinforcement Learning](https://arxiv.org/abs/2508.10874)

**Authors:** Yuchen Zhang, Gang Chen, Ning Ding et al.  
**ArXiv ID:** `2508.10874`  
**Published:** 2025-08-14  
**Organizations:** Tsinghua University, Shanghai Jiao Tong University et al.  
**Votes:** 99 votes  
**Abstract:** We investigate the potential of large language models (LLMs) to serve as efficient simulators for agentic search tasks in reinforcement learning (RL), thereby reducing dependence on costly interaction...

**Categories:** `cs.CL`

---

### 3. [MEMORYLLM: Towards Self-Updatable Large Language Models](https://arxiv.org/abs/2402.04624)

**Authors:** Xian Li, Yu Wang, Zheng Li et al.  
**ArXiv ID:** `2402.04624`  
**Published:** 2024-05-26  
**Organizations:** UC, San Diego, UC, Los Angeles  
**Votes:** 28 votes  
**Abstract:** Existing Large Language Models (LLMs) usually remain static after deployment, which might make it hard to inject new knowledge into the model. We aim to build models containing a considerable portion ...

**Categories:** `cs.CL`

---

</details>

## 📚 Reading

<details open>
<summary><strong>13 papers</strong> - Click to expand</summary>

### 1. [Seeing, Listening, Remembering, and Reasoning: A Multimodal Agent with Long-Term Memory](https://arxiv.org/abs/2508.09736)

**Authors:** Wei Li, Hang Li, Yuan Lin et al.  
**ArXiv ID:** `2508.09736`  
**Published:** 2025-08-15  
**Organizations:** Shanghai Jiao Tong University, Zhejiang University et al.  
**Votes:** 285 votes  
**Abstract:** We introduce M3-Agent, a novel multimodal agent framework equipped with long-term memory. Like humans, M3-Agent can process real-time visual and auditory inputs to build and update its long-term memor...

**Categories:** `cs.CV`

---

### 2. [ComoRAG: A Cognitive-Inspired Memory-Organized RAG for Stateful Long Narrative Reasoning](https://arxiv.org/abs/2508.10419)

**Authors:** Jie Zhou, Wei Wei, Jin Xu et al.  
**ArXiv ID:** `2508.10419`  
**Published:** 2025-08-14  
**Organizations:** N/A  
**Votes:** 23 votes  
**Abstract:** Narrative comprehension on long stories and novels has been a challenging domain attributed to their intricate plotlines and entangled, often evolving relations among characters and entities. Given th...

**Categories:** `cs.CL`, `cs.AI`, `cs.LG`

---

### 3. [DINOv3](https://arxiv.org/abs/2508.10104)

**Authors:** Hervé Jégou, Julien Mairal, Piotr Bojanowski et al.  
**ArXiv ID:** `2508.10104`  
**Published:** 2025-08-13  
**Organizations:** Inria, Meta AI Research et al.  
**Votes:** 196 votes  
**Abstract:** Self-supervised learning holds the promise of eliminating the need for manual data annotation, enabling models to scale effortlessly to massive datasets and larger architectures. By not being tailored...

**Categories:** `cs.CV`, `cs.LG`

---

### 4. [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)

**Authors:** Kai Chen, Qipeng Guo, Bowen Zhou et al.  
**ArXiv ID:** `2508.09874`  
**Published:** 2025-08-13  
**Organizations:** Tsinghua University, Shanghai Jiao Tong University et al.  
**Votes:** 57 votes  
**Abstract:** Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) re...

**Categories:** `cs.CL`, `cs.AI`

---

### 5. [AutoCodeBench: Large Language Models are Automatic Code Benchmark Generators](https://arxiv.org/abs/2508.09101)

**Authors:** Tao Zhang, Ao Liu, Yue Mao et al.  
**ArXiv ID:** `2508.09101`  
**Published:** 2025-08-12  
**Organizations:** Tencent  
**Votes:** 23 votes  
**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains, with code generation emerging as a key area of focus. While numerous benchmarks have been proposed to eva...

**Categories:** `cs.CL`, `cs.SE`

---

### 6. [DeepFleet: Multi-Agent Foundation Models for Mobile Robots](https://arxiv.org/abs/2508.08574)

**Authors:** Ang Li, Scott Niekum, Kyle O'Brien et al.  
**ArXiv ID:** `2508.08574`  
**Published:** 2025-08-12  
**Organizations:** Amazon Robotics  
**Votes:** 11 votes  
**Abstract:** We introduce DeepFleet, a suite of foundation models designed to support coordination and planning for large-scale mobile robot fleets. These models are trained on fleet movement data, including robot...

**Categories:** `cs.RO`, `cs.MA`

---

### 7. [On the Generalization of SFT: A Reinforcement Learning Perspective with Reward Rectification](https://arxiv.org/abs/2508.05629)

**Authors:** Ming-Hsuan Yang, Xinyu Ye, Lu Qi et al.  
**ArXiv ID:** `2508.05629`  
**Published:** 2025-08-07  
**Organizations:** Shanghai Jiao Tong University, UC Berkeley et al.  
**Votes:** 204 votes  
**Abstract:** We present a simple yet theoretically motivated improvement to Supervised Fine-Tuning (SFT) for the Large Language Model (LLM), addressing its limited generalization compared to reinforcement learning...

**Categories:** `cs.LG`

---

### 8. [Agent Lightning: Train ANY AI Agents with Reinforcement Learning](https://arxiv.org/abs/2508.03680)

**Authors:** Yuqing Yang, Zilong Wang, Dongsheng Li et al.  
**ArXiv ID:** `2508.03680`  
**Published:** 2025-08-05  
**Organizations:** Microsoft  
**Votes:** 179 votes  
**Abstract:** We present Agent Lightning, a flexible and extensible framework that enables Reinforcement Learning (RL)-based training of Large Language Models (LLMs) for any AI agent. Unlike existing methods that t...

**Categories:** `cs.AI`, `cs.LG`

---

### 9. [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)

**Authors:** Mingyu Li, Yujun Cai, Jiayu Yao et al.  
**ArXiv ID:** `2507.13334`  
**Published:** 2025-07-21  
**Organizations:** University of Chinese Academy of Sciences, Tsinghua University et al.  
**Votes:** 271 votes  
**Abstract:** The performance of Large Language Models (LLMs) is fundamentally determined by the contextual information provided during inference. This survey introduces Context Engineering, a formal discipline tha...

**Categories:** `cs.CL`

---

### 10. [MemOS: An Operating System for Memory-Augmented Generation (MAG) in  Large Language Models](https://arxiv.org/abs/2505.22101)

**Authors:** Jiawei Yang, Zhi-Qin John Xu, Kai Chen et al.  
**ArXiv ID:** `2505.22101`  
**Published:** 2025-05-28  
**Organizations:** Renmin University of China, Shanghai Jiao Tong University et al.  
**Votes:** 73 votes  
**Abstract:** Large Language Models (LLMs) have emerged as foundational infrastructure in
the pursuit of Artificial General Intelligence (AGI). Despite their remarkable
capabilities in language perception and gener...

**Categories:** `cs.CL`

---

### 11. [Rethinking Memory in AI: Taxonomy, Operations, Topics, and Future  Directions](https://arxiv.org/abs/2505.00675)

**Authors:** Danna Zheng, Mirella Lapata, Jeff Z. Pan et al.  
**ArXiv ID:** `2505.00675`  
**Published:** 2025-05-27  
**Organizations:** The Chinese University of Hong Kong, The University of Edinburgh et al.  
**Votes:** 820 votes  
**Abstract:** Memory is a fundamental component of AI systems, underpinning large language
models (LLMs)-based agents. While prior surveys have focused on memory
applications with LLMs (e.g., enabling personalized ...

**Categories:** `cs.CL`

---

### 12. [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)

**Authors:** Prateek Chhikara, Deshraj Yadav, Taranjeet Singh et al.  
**ArXiv ID:** `2504.19413`  
**Published:** 2025-04-28  
**Organizations:** N/A  
**Votes:** 659 votes  
**Abstract:** Large Language Models (LLMs) have demonstrated remarkable prowess in
generating contextually coherent responses, yet their fixed context windows
pose fundamental challenges for maintaining consistency...

**Categories:** `cs.CL`, `cs.AI`

---

### 13. [Memory Consolidation Enables Long-Context Video Understanding](https://arxiv.org/abs/2402.05861)

**Authors:** Rahma Chaabouni, Yuge Shi, Skanda Koppula et al.  
**ArXiv ID:** `2402.05861`  
**Published:** 2024-05-31  
**Organizations:** Google DeepMind  
**Votes:** 10 votes  
**Abstract:** Most transformer-based video encoders are limited to short temporal contexts due to their quadratic complexity. While various attempts have been made to extend this context, this has often come at the...

**Categories:** `cs.CV`

---

</details>

## 📖 Want to read

> No papers yet.


---

<div align="center">

**Last Updated:** 2025-08-22 00:21:09 UTC

*This file is automatically generated by the [format script](./scripts/format.ts)*

</div>
